---
title: "Implementing the optimism-adjusted bootstrap with tidymodels"
author: "Patrick Rockenschaub"
date: 2022-01-26T12:00:00-00:00
categories: ["R"]
tags: ["predictive modelling", "model validation", "tidymodels"]
bibliography: references.bib
---



<p>It is well known that prediction models have a tendency to overfit to the training data, especially if we only have a limited amount of training data. While performance of such overfitted models appears high when evaluated on the data available during training, their performance on new, previously unseen data is often considerably worse. Although it may be tempting to the analyst to choose a model with high training performance, it is the model’s performance in future data that we are really interested in.</p>
<p>Several resampling methods have been proposed to account for this issue. The most widely used techniques fall into two categories: cross-validation and bootstrapping. The idea underlying these techniques is similar. By repeating the model fitting multiple times on different subsets of the training data, we may get a better understanding of the magnitude of overfitting and can account for it in our model building and evaluation. Without going into too much detail, cross-validation separates the data into <span class="math inline">\(k\)</span> mutually exclusive folds and always holds one back as a “hidden” test set. Note that the sample size available to the model during each training run necessarily decreases to <span class="math inline">\(\frac{k-1}{k}n\)</span>. Bootstrap, on the other hand, resamples (with replacement) a data set with the same size <span class="math inline">\(n\)</span> as the original training set and then — depending on the exact method — uses a weighted combination randomly sampled and excluded observations.</p>
<p>Whereas the machine learning community almost exclusively uses cross-validation for model validation, bootstrap-based methods may be more commonly seen in biomedical sciences. One reason for this popularity may be the fact that they are championed by preeminent experts in the field: both Frank Harrell <span class="citation">(<a href="#ref-Harrell2015-ws" role="doc-biblioref">Harrell 2015</a>)</span> and Ewout Steyerberg <span class="citation">(<a href="#ref-Steyerberg2019-yc" role="doc-biblioref">Steyerberg 2019</a>)</span> prominently feature the bootstrap — and in particular the optimism-adjusted bootstrap (OAD) — in their textbooks. In this post, I give a brief introduction into OAD and compare it to repeated cross-validation and regular bootstrap. OAD is implemented in the R packages <a href="https://cran.r-project.org/web/packages/caret/"><em>caret</em></a> and Frank Harrell’s <a href="https://cran.r-project.org/web/packages/rms/"><em>rms</em></a> but not in the recent <a href="https://cran.r-project.org/web/packages/tidymodels/"><em>tidymodels</em></a> ecosystem <span class="citation">(<a href="#ref-Kuhn2022-al" role="doc-biblioref">Kuhn and Silge 2022</a>)</span>. This post will therefore provide a step-by-step guide to doing OAD with <a href="https://cran.r-project.org/web/packages/tidymodels/"><em>tidymodels</em></a>.</p>
<div id="who-this-post-is-for" class="section level1">
<h1>Who this post is for</h1>
<p>Here’s what I assume you to know:</p>
<ul>
<li>You’re familiar with <a href="https://www.r-project.org/">R</a> and the <a href="https://www.tidyverse.org/">tidyverse</a>, including the amazing <a href="https://www.tidymodels.org/">tidymodels</a> framework (if not go check it out now!).</li>
<li>You know a little bit about fitting and evaluating linear regression models.</li>
</ul>
<p>We will use the following <a href="https://www.r-project.org/">R</a> packages throughout this post:</p>
<pre class="r"><code>library(tidyverse)
library(tidymodels)</code></pre>
</div>
<div id="optimism-adjusted-bootstrap" class="section level1">
<h1>Optimism-adjusted bootstrap</h1>
<p>Like other resampling schemes, the OAD aims to avoid overly optimistic estimation of model performance during internal validation — i.e., validation of model performance using the training dataset. As we will see further down, simply calculating performance metrics on the same data used for training leads to artificially high/good performance estimates. We will call this the “apparent” performance. OAD proposes to obtain a better estimate by directly estimating the amount of “optimism” in the apparent performance. The steps needed to do so are as follows <span class="citation">(<a href="#ref-Steyerberg2019-yc" role="doc-biblioref">Steyerberg 2019</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>Fit a model <span class="math inline">\(M\)</span> to the original training set <span class="math inline">\(S\)</span> and use <span class="math inline">\(M\)</span> to calculate the apparent performance <span class="math inline">\(R(M, S)\)</span> (e.g., accuracy) on the training data</li>
<li>Draw a bootstrapped sample <span class="math inline">\(S^*\)</span> of the same size as <span class="math inline">\(S\)</span> through sampling <em>with</em> replacement</li>
<li>Construct another model <span class="math inline">\(M*\)</span> by performing all model building steps (pre-processing, imputation, model selection, etc.) on <span class="math inline">\(S^*\)</span> and calculate it’s apparent performance <span class="math inline">\(R(M^*, S^*)\)</span> on <span class="math inline">\(S*\)</span></li>
<li>Use <span class="math inline">\(M*\)</span> to estimate the performance <span class="math inline">\(R(M^*, S)\)</span> that it would have had on the original data <span class="math inline">\(S\)</span>.</li>
<li>Calculate the optimism <span class="math inline">\(O^* = R(M^*, S^*) - R(M^*, S)\)</span> as the difference between the apparent and test performance of <span class="math inline">\(M*\)</span>.</li>
<li>Repeat steps 2.-5. may times <span class="math inline">\(B\)</span> to obtain a sufficiently stable estimate (common recommendations range from 100-1000 times depending on the computational feasibility)</li>
<li>Subtract the mean optimism <span class="math inline">\(\frac{1}{B} \sum^B_{b=1} O^*_b\)</span> from the apparent performance <span class="math inline">\(R_{app}\)</span> in the original training data <span class="math inline">\(S\)</span> to get a optimism-adjusted estimate of model performance.</li>
</ol>
<p>The basic intuition behind this procedure is that the model <span class="math inline">\(M*\)</span> will overfit to <span class="math inline">\(S^*\)</span> in the same way as <span class="math inline">\(M\)</span> overfits to <span class="math inline">\(S\)</span>. We can then estimate the difference between <span class="math inline">\(M\)</span>’s observed apparent performance <span class="math inline">\(R(M, S)\)</span> and its unobserved performance on future test data <span class="math inline">\(R(M, U)\)</span> from the difference between the bootstrapped model <span class="math inline">\(M^*\)</span>’s apparent performance <span class="math inline">\(R(M^*, S^*)\)</span> and its test performance <span class="math inline">\(R(M^*, S)\)</span> (which are both observed). The training data <span class="math inline">\(S\)</span> acts as a stand-in test data for the bootstrapped model <span class="math inline">\(M*\)</span>.</p>
<p>The following sections will apply this basic idea to the Ames housing dataset and compare estimates derived via OAB to repeated cross-validation and standard bootstrap.</p>
</div>
<div id="the-ames-data-set" class="section level1">
<h1>The Ames data set</h1>
<p>The Ames data set contains information on 2,930 properties in Ames, Iowa, and contains 74 variables including the number of bedrooms, whether the property includes a garage, and the sale price. We choose this data set because it provides a decent sample size for predictive modelling and is already used prominently in the documentation of the R <code>tidymodels</code> ecosystem. More information on the Ames data set can be found in <span class="citation">(<a href="#ref-Kuhn2022-al" role="doc-biblioref">Kuhn and Silge 2022</a>)</span>.</p>
<pre class="r"><code>set.seed(123)

data(ames)
dim(ames)</code></pre>
<pre><code>## [1] 2930   74</code></pre>
<pre class="r"><code>ames[1:5, 1:5]</code></pre>
<pre><code>## # A tibble: 5 × 5
##   MS_SubClass                         MS_Zoning     Lot_Frontage Lot_Area Street
##   &lt;fct&gt;                               &lt;fct&gt;                &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt; 
## 1 One_Story_1946_and_Newer_All_Styles Residential_…          141    31770 Pave  
## 2 One_Story_1946_and_Newer_All_Styles Residential_…           80    11622 Pave  
## 3 One_Story_1946_and_Newer_All_Styles Residential_…           81    14267 Pave  
## 4 One_Story_1946_and_Newer_All_Styles Residential_…           93    11160 Pave  
## 5 Two_Story_1946_and_Newer            Residential_…           74    13830 Pave</code></pre>
<p>For this exercise, we try to predict sale prices within the dataset. To keep preprocessing simple, we limit the predictors to only numeric variables, which we centre and scale. Since sale prices are right skewed, we log them before prediction. Finally, we will hold back a random quarter of the data to simulate external validation on an independent identically distributed test set.</p>
<pre class="r"><code># Define sale price as the prediction target
formula &lt;- Sale_Price ~ .

# Remove categorical variables, log sale price, scale the numeric predictors
preproc &lt;- recipe(formula, data = ames[0, ]) %&gt;% 
  step_rm(all_nominal_predictors()) %&gt;% 
  step_log(all_outcomes()) %&gt;% 
  step_normalize(all_numeric_predictors(), -all_outcomes())

# Randomly split into training (3/4) and testing (1/4) sets
train_test_split &lt;- initial_split(ames, prop = 3/4)
train &lt;- training(train_test_split)
test &lt;- testing(train_test_split)</code></pre>
</div>
<div id="optimism-adjusted-bootstrap-with-tidymodels" class="section level1">
<h1>Optimism-adjusted bootstrap with <em>tidymodels</em></h1>
<p>Now that we have set up the data, lets look into how we can build a linear regression model and validate it via OAB. We proceed according to the steps described above.</p>
<div id="step-1-calculate-apparent-perforamnce" class="section level2">
<h2>Step 1: Calculate apparent perforamnce</h2>
<p>To start, we simply fit and evaulate our model <span class="math inline">\(M\)</span> on the original training data <span class="math inline">\(S\)</span> (note that we also apply preprocessing, therefore we strictly speaking train our model on the preprocessed data <span class="math inline">\(S&#39;\)</span>). Since our outcome is a continuous value strictly greater than zero, we will use the residual mean squared error as our performance metric.</p>
<pre class="r"><code>prepped &lt;- prep(preproc, train)
preproc_orig &lt;- juice(prepped)
fit_orig &lt;- fit(linear_reg(), formula, preproc_orig)
preds_orig &lt;- predict(fit_orig, new_data = preproc_orig)
perf_orig &lt;- rmse_vec(preproc_orig$Sale_Price, preds_orig$.pred)

perf_orig</code></pre>
<pre><code>## [1] 0.1693906</code></pre>
</div>
<div id="step-2-create-bootstrapped-samples" class="section level2">
<h2>Step 2: Create bootstrapped samples</h2>
<p>After obtaining <span class="math inline">\(M\)</span> and <span class="math inline">\(R(M, S)\)</span>, we now produce a set of bootstrap samples to estimate the amount of optimism in this performance estiamte. We use the <em>tidymodels</em> sub-package <em>rsample</em> to create a data frame <code>bs</code> with <code>200</code> bootstrap samples. All of these resamples have training data of equal size to the original training data (n = 2197). Note however that the “testing data” set aside differs between splits, as it is defined by all rows that did not get sampled into the training data, which is a random variable and may vary between bootstraps. We won’t use this testing data for OAB but it is for example used in the simple bootstrap that we use for comparison later.</p>
<pre class="r"><code>bs &lt;- bootstraps(train, times = 100L)

bs %&gt;% slice(1:5)</code></pre>
<pre><code>## # A tibble: 5 × 2
##   splits             id          
##   &lt;list&gt;             &lt;chr&gt;       
## 1 &lt;split [2197/813]&gt; Bootstrap001
## 2 &lt;split [2197/818]&gt; Bootstrap002
## 3 &lt;split [2197/813]&gt; Bootstrap003
## 4 &lt;split [2197/786]&gt; Bootstrap004
## 5 &lt;split [2197/792]&gt; Bootstrap005</code></pre>
<pre class="r"><code>bs %&gt;% slice((n()-5):n())</code></pre>
<pre><code>## # A tibble: 6 × 2
##   splits             id          
##   &lt;list&gt;             &lt;chr&gt;       
## 1 &lt;split [2197/806]&gt; Bootstrap095
## 2 &lt;split [2197/822]&gt; Bootstrap096
## 3 &lt;split [2197/778]&gt; Bootstrap097
## 4 &lt;split [2197/800]&gt; Bootstrap098
## 5 &lt;split [2197/833]&gt; Bootstrap099
## 6 &lt;split [2197/815]&gt; Bootstrap100</code></pre>
</div>
<div id="step-3-fit-bootstrapped-models-and-calculate-their-apparent-performance" class="section level2">
<h2>Step 3: Fit bootstrapped models and calculate their apparent performance</h2>
<p>We now use the bootstrap data.frame <code>bs</code> to preprocess each sample <span class="math inline">\(S^*\)</span> individually, fit a linear regression <span class="math inline">\(M^*\)</span> to it, and calculate its apparent performance <span class="math inline">\(R(M^*, S^*)\)</span>.</p>
<pre class="r"><code>bs &lt;- bs %&gt;% 
  mutate(
    # Apply preprocessing separately for each bootstrapped sample S*
    processed = map(splits, ~ juice(prep(preproc, training(.)))),
    # Fit a separate model M* to each preprocessed bootstrap
    fitted = map(processed, ~ fit(linear_reg(), formula, data = .)),
    # Predict values for each bootstrap&#39;s training data S* and calculate RMSE
    pred_app = map2(fitted, processed, ~ predict(.x, new_data = .y)),
    perf_app = map2_dbl(processed, pred_app, ~ rmse_vec(.x$Sale_Price, .y$.pred))
  )</code></pre>
</div>
<div id="step-4-evaluate-on-the-original-training-data" class="section level2">
<h2>Step 4: Evaluate on the original training data</h2>
<p>Since we stored the fitted models <span class="math inline">\(M^*_i\)</span> in a column of the data.frame, we can easily re-use them to predict values for the original data and evaluate them. Remember that because some of the rows in the original dataset did not end up in the bootstrapped dataset, we expect the performance <span class="math inline">\(R(M^*_i, S)\)</span> of each model <span class="math inline">\(M^*_i\)</span> to be lower than the performance in its own training data <span class="math inline">\(R(M^*_i, S^*_i)\)</span>.</p>
<pre class="r"><code>bs &lt;- bs %&gt;% 
  mutate(
    pred_test = map(fitted, ~ predict(., new_data = preproc_orig)),
    perf_test = map_dbl(pred_test, ~ rmse_vec(preproc_orig$Sale_Price, .$.pred)),
  )</code></pre>
</div>
<div id="step-5-estimate-the-optimism" class="section level2">
<h2>Step 5: Estimate the optimism</h2>
<p>The amount of optimism in our apparent estimate is now simply estimated by the differences between apparent and test performance in each bootstrap.</p>
<pre class="r"><code>bs &lt;- bs %&gt;% 
  mutate(
    optim = perf_app - perf_test
  )</code></pre>
</div>
<div id="steps-6-7-adjust-for-optimism" class="section level2">
<h2>Steps 6-7: Adjust for optimism</h2>
<p>We already repeated this procedure in parallel for 200 samples, therefore step 6 is fulfilled. In order to get a single, final estimate, all that’s left to do is to calculate the mean and standard deviation of the optimism and substract them (which approximately normal 95% Wald confidence limits) from the apparent performance obtained in step 1. This is now the performance that we report for our model after internal validation</p>
<pre class="r"><code>mean_opt &lt;- mean(bs$optim)
std_opt &lt;- sd(bs$optim)

(perf_orig - mean_opt) + c(-2, 0, 2) * std_opt / sqrt(nrow(bs))</code></pre>
<pre><code>## [1] 0.1766124 0.1789846 0.1813568</code></pre>
</div>
<div id="external-validation" class="section level2">
<h2>External validation</h2>
<p>Remember that we set aside a quarter of the data for external validation (external is a bit of misnomer here but more on that later). We can now compare how our estimate from internal validation compares to the performance in the held-out data. Indeed, the performance seems to have slightly dropped but — thankfully — it is still within the bounds suggested by OAB above.</p>
<pre class="r"><code>preproc_test &lt;- bake(prepped, test)
preds_test &lt;- predict(fit_orig, new_data = preproc_test)
rmse_vec(preproc_test$Sale_Price, preds_test$.pred)</code></pre>
<pre><code>## [1] 0.1855153</code></pre>
</div>
</div>
<div id="putting-everything-together" class="section level1">
<h1>Putting everything together</h1>
<p>Using what we learned above, we can create a single function <code>calculate_optimism_adjusted()</code> that performs all steps and returns the adjusted model performance.</p>
<pre class="r"><code>calculate_optimism_adjusted &lt;- function(train_data, formula, preproc, n_resamples = 10L) {
  # Get apparent performance
  prepped &lt;- prep(preproc, train_data)
  preproc_orig &lt;- juice(prepped)
  fit_orig &lt;- fit(linear_reg(), formula, preproc_orig)
  preds_orig &lt;- predict(fit_orig, new_data = preproc_orig)
  perf_orig &lt;- rmse_vec(last(preproc_orig), preds_orig$.pred)
  
  # Estimate optimism via bootstrap
  rsmpl &lt;- bootstraps(train_data, times = n_resamples) %&gt;% 
    mutate(
      processed = map(splits, ~ juice(prep(preproc, training(.)))),
      fitted = map(processed, ~ fit(linear_reg(), formula, data = .)),
      pred_app = map2(fitted, processed, ~ predict(.x, new_data = .y)),
      perf_app = map2_dbl(processed, pred_app, ~ rmse_vec(.x$Sale_Price, .y$.pred)),
      pred_test = map(fitted, ~ predict(., new_data = preproc_orig)),
      perf_test = map_dbl(pred_test, ~ rmse_vec(last(preproc_orig), .$.pred)),
      optim = perf_app - perf_test
    )
  
  mean_opt &lt;- mean(rsmpl$optim)
  std_opt &lt;- sd(rsmpl$optim)

  # Adjust for optimism
  tibble(
    .metric = &quot;rmse&quot;,
    mean = perf_orig - mean_opt, 
    n = n_resamples, 
    std_err = std_opt / sqrt(n_resamples)
  )
}</code></pre>
<p>We also define a similar function <code>eval_test</code> for the external validation and wrappers around <em>tidymodel</em>’s <code>fit_resample</code> to do the same for repeated cross-validation (<code>calculate_repeated_cv()</code>) and standard bootstrap (<code>calculate_standard_bs()</code>), which we will compare in a second.</p>
<pre class="r"><code>eval_test &lt;- function(train_data, test_data, formula, preproc) {
  
  prepped &lt;- prep(preproc, train_data)
  preproc_train &lt;- juice(prepped)
  preproc_test &lt;- bake(prepped, test_data)
  fitted &lt;- fit(linear_reg(), formula, data = preproc_train)
  preds &lt;- predict(fitted, new_data = preproc_test)
  rmse_vec(preproc_test$Sale_Price, preds$.pred)
}

calculate_repeated_cv &lt;- function(train_data, formula, preproc, v = 10L, repeats = 1L){
  rsmpl &lt;- vfold_cv(train_data, v = v, repeats = repeats)

  show_best(fit_resamples(linear_reg(), preproc, rsmpl), metric = &quot;rmse&quot;) %&gt;% 
    select(-.estimator, -.config)
}

calculate_standard_bs &lt;- function(train_data, formula, preproc, n_resamples = 10L) {
  rsmpl &lt;- bootstraps(train_data, times = n_resamples, apparent = FALSE)

  show_best(fit_resamples(linear_reg(), preproc, rsmpl), metric = &quot;rmse&quot;) %&gt;% 
    select(-.estimator, -.config)
}</code></pre>
</div>
<div id="comparison-of-validation-methods" class="section level1">
<h1>Comparison of validation methods</h1>
<p>In this last section, we will compare the results obtained from OAB to two other well-known validation methods: repeated 10-fold cross-validation and standard bootstrap. In the former, we randomly split the data into 10 mutually exclusive folds of equal size. In a round-robin fashion, we set aside one fold as an evaluation set and use the remaining nine to train our model. We then choose the next fold and do the same. After one round, we have ten estimates of model performance, one for each held-out fold. We repeat this process several times with new random seeds to get the same number of resamples as were used for the bootstrap. With standard bootstrap, we fit our models on the same bootstrapped data but evaluate them on the samples that were randomly excluded from that particular bootstrap — similar to the held-out fold of cross-validation.</p>
<p>In order to get a good comparison of methods, we won’t stick with a single train-test split as before but use nested validation. The reason for this is that our test data isn’t truly external. Instead, it is randomly sampled from the entire development dataset (which in this case was all of Ames). Holding out a single chunk of that data as test data would be wasteful and could again result in us being particularly lucky or unlucky in the selection of that chunk. This is particularly problematic if we further perform hyperparameter searches. In nested validation, we mitigate this risk by wrapping our entire internal validation in another cross-validation loop, i.e., we treat the held-out set of an outer cross-validation as the “external” test set.</p>
<pre class="r"><code>outer &lt;- vfold_cv(ames, v = 5, repeats = 1)

outer &lt;- outer %&gt;% 
  mutate(
    opt = splits %&gt;% 
      map(~ calculate_optimism_adjusted(training(.), formula, preproc, 10L)),
    cv = splits %&gt;% 
      map(~ calculate_repeated_cv(training(.), formula, preproc, repeats = 1L)), 
    bs = splits %&gt;% 
      map(~ calculate_standard_bs(training(.), formula, preproc, 10L)), 
    test = splits %&gt;% 
      map_dbl(~ eval_test(training(.), testing(.), formula, preproc))
  )</code></pre>
<p>We can see below that in this example, all resampling methods perform more or less similar. Notably, both bootstrap-based methods have narrower confidence intervals. This was to be expected, as cross-validation typically has high variance. This increased precision is traded for a risk of bias in bootstrap, which is usually pessimistic as with the standard bootstrap in this example. OAB here seems to have a slight optimistic bias. While its mean is similar to cross-validation, its increased confidence represented by narrower confidence interval means that the average test performance over the nested runs is not contained in the approximate confidence limits. However, all resampling methods give us a more accurate estimate of likely future model performance than the apparent performance of 0.169.</p>
<pre class="r"><code>format_results &lt;- function(outer, method) {
  method &lt;- rlang::enquo(method)
  
  outer %&gt;% 
    unnest(!!method) %&gt;% 
    summarise(
      rsmpl_lower = mean(mean - 2 * std_err),
      rsmpl_mean  = mean(mean), 
      rsmpl_upper = mean(mean + 2 * std_err), 
      test_mean   = mean(test)
    )
}

tibble(method = c(&quot;opt&quot;, &quot;cv&quot;, &quot;bs&quot;)) %&gt;% 
  bind_cols(bind_rows(
    format_results(outer, opt), 
    format_results(outer, cv), 
    format_results(outer, bs), 
  ))</code></pre>
<pre><code>## # A tibble: 3 × 5
##   method rsmpl_lower rsmpl_mean rsmpl_upper test_mean
##   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;
## 1 opt          0.171      0.178       0.185     0.179
## 2 cv           0.152      0.177       0.202     0.179
## 3 bs           0.173      0.182       0.191     0.179</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Harrell2015-ws" class="csl-entry">
Harrell, Frank E, Jr. 2015. <em>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</em>. Springer, Cham.
</div>
<div id="ref-Kuhn2022-al" class="csl-entry">
Kuhn, Max, and Julia Silge. 2022. <em>Tidy Modeling with r</em>. <a href="https://www.tmwr.org/">https://www.tmwr.org/</a>.
</div>
<div id="ref-Steyerberg2019-yc" class="csl-entry">
Steyerberg, Ewout W. 2019. <em>Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating</em>. Springer, Cham.
</div>
</div>
</div>
