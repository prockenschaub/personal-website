---
title: "Contextual nature of AUC"
summary: "The area under the receiver operating characteristic (AUC) is arguably among the most frequently used measures of classification performance. In this post, I look at how performance estimated by AUC changes based on the covariate distribution in the evaluation set, even if we know the true model. This complicates the interpretation of external validation results if a change in this distribution is possible or even likely."
authors: 
  - patrick
date: 2021-11-26T21:13:14-05:00
categories: ["R"]
tags: ["predictive modelling", "generalisation", "model validation"]
---



<p>The area under the receiver operating characteristic (AUC) is arguably among
the most frequently used measures of classification performance. Unlike other
common measures like sensitivity, specificity, or accuracy, the AUC does not
require (often arbitrary) thresholds. It also lends itself to a very simple
and intuitive interpretation: a models AUC equals the probability that, for any
randomly chosen pair with and without the outcome, the observation with the
outcome is assigned a higher risk score by the model than the observation
without the outcome, or</p>
<p><span class="math display">\[
P(f(x_i) &gt; f(x_j))
\]</span></p>
<p>where <span class="math inline">\(f(x_i)\)</span> is the risk score that the model
assigned to observation <span class="math inline">\(i\)</span> based on its covariates <span class="math inline">\(x_i\)</span>,
<span class="math inline">\(i \in D_{y=1}\)</span> is an observation taken from among all cases <span class="math inline">\(y=1\)</span>, and
<span class="math inline">\(j \in D_{y=0}\)</span> is an observation taken from among all controls <span class="math inline">\(y=0\)</span>. As such,
the AUC has a nice probabilistic meaning and can be linked back to the
well-known <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves">Mann-Whitney U test</a>.</p>
<p>The ubiquitous use of AUC isn’t without controversy (which, like so
many things these days, spilled over into
<a href="https://twitter.com/cecilejanssens/status/1104134423673479169">Twitter</a>).
Regularly voiced criticisms of AUC — and the closely linked receiver operating
characteristic (ROC) curve — include its indifference to class imbalance and
extreme observations.</p>
<p>In this post, I want to take a closer look at a feature of AUC that — at least
in my experience — is often overlooked when evaluating models in an external
test set: the dependence of the AUC on the distribution of variables in the
test set. We will see that this can lead to considerable changes in estimated
AUC even if our model is actually correct, and make it harder to disentangle
changes in performance due to model misspecification from changes in performance
due to differences between development and test sets.</p>
<div id="who-this-post-is-for" class="section level1">
<h1>Who this post is for</h1>
<p>Here’s what I assume you to know:</p>
<ul>
<li>You’re familiar with <a href="https://www.r-project.org/">R</a> and the <a href="https://www.tidyverse.org/">tidyverse</a>.</li>
<li>You know a little bit about fitting and evaluating linear regression models.</li>
</ul>
<p>We will use the following <a href="https://www.r-project.org/">R</a> packages throughout this post:</p>
<pre class="r"><code>library(MASS)
library(tidyverse)
library(tidymodels)</code></pre>
</div>
<div id="generating-some-dummy-data" class="section level1">
<h1>Generating some dummy data</h1>
<p>Let’s get started by
simulating some fake medical data for 100,000 patients. Assume we are
interested in predicting their probability of death (=outcome). We use the
patients’ sex (binary) and three continuous measurements — age, blood pressure
, and cholesterol — to do so. In this fake data set, being female has a strong
protective effect (odds ratio = exp(-2) = 0.14) and all other variables have a
moderate effect (odds ratio per standard deviation = exp(0.3) = 1.35).
Any influence by other, unmeasured factors is simulated by drawing from a
Bernoulli distribution with a probability defined by sex, age, blood pressure,
and cholesterol.</p>
<pre class="r"><code>set.seed(42)

# Set number of rows and predictors
n &lt;- 100000
p &lt;- 3

# Simulate an additional binary predictor (e.g., sex)
sex &lt;- rep(0:1, each = n %/% 2)

# Simulate multivariate normal predictors (e.g., age, blood pressure, 
# cholesterol)
mu &lt;- rep(0, p)
Sigma &lt;- 0.8 * diag(p) + 0.2 * matrix(1, p, p)
other_covars &lt;- MASS::mvrnorm(n, mu, Sigma)
colnames(other_covars) &lt;- c(&quot;age&quot;, &quot;bp&quot;, &quot;chol&quot;)

# Simulate binary outcome (e.g., death)
logistic &lt;- function(x) 1 / (1 + exp(-x))
betas &lt;- c(0.8, -2, 0.3, 0.3, 0.3)
lp &lt;- cbind(1, sex, other_covars) %*% betas
death &lt;- rbinom(n, 1, logistic(lp)) 

# Make into a data.frame and split into a training set (first half) and 
# a biased test set (rows in the second half X * beta &gt; 0)
data &lt;- as_tibble(other_covars)
data$sex &lt;- factor(sex, 0:1, c(&quot;male&quot;, &quot;female&quot;))
data$pred_risk &lt;- as.vector(logistic(lp))
data$death &lt;- factor(death, 0:1, c(&quot;no&quot;, &quot;yes&quot;))
data$id &lt;- 1:nrow(data)</code></pre>
</div>
<div id="estimating-predictive-performance" class="section level1">
<h1>Estimating predictive performance</h1>
<p>Now that we have some data, we can evaluate how well our model is able to
predict each patient’s risk of death. To make our lives as simple as possible,
we assume that we were able to divine the true effects of each of our variables,
i.e., we know that the data is generated via a logistic regression model with
<span class="math inline">\(\beta = [0.8, -2, 0.3, 0.3, 0.3]\)</span> and there is no uncertainty around those
estimates. Under these assumptions, our model would be able to achieve the
following AUC in the simulated data.</p>
<pre class="r"><code>auc &lt;- function(data) {
  yardstick::roc_auc(data, death, pred_risk, event_level = &quot;second&quot;)
}

data %&gt;% auc()</code></pre>
<pre><code>## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.785</code></pre>
<p>Notably, this is a summary measure that depends on the entire data set <span class="math inline">\(D\)</span> and
<em>cannot</em> be calculated for an individual patient alone. Per definition, it
requires at least one patient with and without the outcome. This has important
implications for interpreting the AUC. Let’s see what happens if we evaluate
our (true) model in men and women separately.</p>
<pre class="r"><code>men &lt;- data %&gt;% filter(sex == &quot;male&quot;)
men %&gt;% auc()</code></pre>
<pre><code>## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.662</code></pre>
<pre class="r"><code>women &lt;- data %&gt;% filter(sex == &quot;female&quot;)
women %&gt;% auc()</code></pre>
<pre><code>## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.665</code></pre>
<p>In each subset, the AUC dropped from 0.78 to around 0.66. This perhaps isn’t
too surprising, given that sex was a strong predictor of death.
However, remember that the model coefficients and hence the predicted risk for
each individual patient — i.e., how “good” that prediction is for that patient
— remain unchanged. We merely changed the set of patients that we included in
the evaluation. Although this might be obvious, I believe this is an important
point to highlight.</p>
<p>Looking at the distribution of risks in the total data and by sex might provide
some further intuition for this finding. Predicted risks for both those who did
and did not die are clearly bimodal in the total population around the average
risk for men and women (Figure <a href="#fig:dist-of-risks-overall">1</a>). Even so, there
is good separation between them. The majority of patients who died (red curve)
had a predicted risk &gt;50%. Vice versa, the majority of patients who remained
alive (green curve) had a risk &lt;50%. Looking at the risks of men and women
separately, however, we can see that most men had a high and most women a low
predicted risk (Figure <a href="#fig:dist-of-risks-by-sex">2</a>). There is much less
separation between the red and green curves, as any differences among for
example the men is entirely due to moderate effects of our simulated continuous
covariates.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dist-of-risks-overall"></span>
<img src="{{< blogdown/postref >}}index.en_files/figure-html/dist-of-risks-overall-1.png" alt="Distribution of predicted risk of death for patients that ultimately did and did not die." width="672" />
<p class="caption">
Figure 1: Distribution of predicted risk of death for patients that ultimately did and did not die.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dist-of-risks-by-sex"></span>
<img src="{{< blogdown/postref >}}index.en_files/figure-html/dist-of-risks-by-sex-1.png" alt="Distribution of predicted risk of death for by sex." width="672" />
<p class="caption">
Figure 2: Distribution of predicted risk of death for by sex.
</p>
</div>
<p>So far, we have looked at two extremes: the entire data set, in which sex was
perfectly balanced, and two completely separated subsets with only men or only
women. Let’s see what would happen if we gradually reduce the number of men in
our evaluation population. We can see that estimated performance drops as we
remove more and more men from the data set
(Figure <a href="#fig:plot-auc-under-shift">3</a>),
particularly at the right side of the graph when there are only few men left and
there is an increasing sex imbalance.</p>
<pre class="r"><code>auc_biased &lt;- function(data, p_men_remove) {
  n_men &lt;- sum(data$sex == &quot;male&quot;)
  n_exclude &lt;- floor(n_men * p_men_remove)
  
  data %&gt;% 
    slice(-(1:n_exclude)) %&gt;% 
    auc()
}

p_men_remove &lt;- seq(0, 1, by = 0.01)
auc_shift &lt;- p_men_remove %&gt;% 
  map_dfr(auc_biased, data = data) %&gt;% 
  mutate(p_men_remove = p_men_remove)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plot-auc-under-shift"></span>
<img src="{{< blogdown/postref >}}index.en_files/figure-html/plot-auc-under-shift-1.png" alt="Estimated AUC by proportion of men removed from the full data set." width="672" />
<p class="caption">
Figure 3: Estimated AUC by proportion of men removed from the full data set.
</p>
</div>
</div>
<div id="difficulty-to-distinguish-model-misspecification" class="section level1">
<h1>Difficulty to distinguish model misspecification</h1>
<p>What we have seen so far is particularly problematic in the interpretation of
external model validation, i.e., when we test a model that was developed in one
set of patients (and potentially overfit to that population) in another patient
population in order to estimate the model’s likely future performance. This is
because in most real world cases, it isn’t quite as straightforward to quantify
the difference between the development population and the evaluation cohort.
Since we also usually don’t know the true model parameters (or even the true
model class), it is difficult to disentangle the effects of population makeup
from the effects of model misspecification. Let’s assume for example that —
unlike earlier — we don’t know the exact model parameters and instead needed
to estimate them from a prior development data set. As a result, we obtained
<span class="math inline">\(\beta_{biased} = [0.8, -2, 0.3, 0.3, 0.3]\)</span> which clearly differs from the true
<span class="math inline">\(\beta\)</span> used to generate the data. Now recalculate the AUC in a data set where
there is some imbalance between men and women.</p>
<pre class="r"><code>biased_betas &lt;- c(0.4, -1, 0.3, -0.2, 0.5)
alt_risk &lt;- logistic(cbind(1, sex, other_covars) %*% biased_betas) %&gt;% 
  as.vector()

data %&gt;% 
  mutate(pred_risk = alt_risk) %&gt;% 
  auc_biased(0.5)</code></pre>
<pre><code>## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.722</code></pre>
<p>Again, the estimated model performance dropped. However, how much of the drop
was due to our biased estimate <span class="math inline">\(\beta_{biased} \neq \beta\)</span> and how much of it
was due to the fact that our evaluation data set contained fewer men? This, in
general, is not straightforward to answer.</p>
</div>
<div id="takeaway" class="section level1">
<h1>Takeaway</h1>
<p>If there is one takeaway from this post it is that external validations of
predictive models mustn’t solely report on differences in AUC but also need to
comment on the comparability of development and test sets used. Such a
discussion is warranted irrespective of whether the performance remained the
same, dropped, or even increased in the test set. Only by discussion — and
ideally even quantifying — differences between the data sets can the reader
fully assess the evidence for retained model performance and judge its likely
value in the future.</p>
</div>
